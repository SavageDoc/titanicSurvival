---
title: "Support Vector Machines for Titanic Survival"
author: 'Craig "Doc" Savage'
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: yes
    toc_depth: 2
    code_folding: show
---

```{r setup, include=TRUE, echo=FALSE}
## Settings
knitr::opts_chunk$set( message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE )

## Constants
# Probability of correct classification of baseline model
# In this case, the baseline model assumes survival if and only if the passenger is female.
BASELINE_PCC = scales::percent( 0.76555 )
# Rule-based
RULE_PCC = scales::percent( 0.67464 )
EXPERT_PCC = RULE_PCC
# First cut with RBF and "reasonable" settings
SVM_PCC1 = scales::percent( 0.78947 )
# Expected overtrained SVM
SVM_PCC_WORST = scales::percent( 0.71291 )
# Restricting the variables
SVM_SMALL_PCC = scales::percent( 0.78947 )
# And the metric from my SVM on public test data/leaderboard
SVM_PCC_BEST = scales::percent( 0.79425 )
```

# Executive Summary

I had a go at utilising support vector machines (SVM) on the Titanic Survival data, with as little wrangling of the data as feasible. It seems that the algorithm is quite consistent - various parameter settings resulted in public test set results of `r SVM_PCC_WORST` to `r SVM_PCC_BEST`. Oddly, this is in-line with the simple prediction algorithm of *A passenger survived if and only if they were female*, at `r BASELINE_PCC`.

The point of this kernel is to show that complexity does not always yield better results. Obviously, as this is a historical data set, it is possible to have a perfect score without any classification whatsoever. This is acknowledged in the [Rules](https://www.kaggle.com/c/titanic/rules) as given:

> This is a fun competition aimed at helping you get started with machine learning.  While the Titanic dataset is publically available on the internet, looking up the answers defeats the entire purpose.  So seriously, don't do that.

# Introduction{.tabset .tabset-fade .tabset-pills}

Kaggle hosts a number of competitions - most famously those that offer monetary rewards to the "best" model submissions. In addition, Kaggle hosts ongoing competitions, as a common reference point for beginners and experts to practice. As an ongoing competition, there are unlimited entries with no final end date: a "good" score may be reached via optimisation rather than classification, even without resorting to predictions based on passenger manifests available on the Internet.

Before beginning the classification task, it is important to consider the difference between classification and optimisation tasks: after all, determining a classification algorithm will depend on an optimisation. In essence, the *spirit* of the solution is the important part. By solving the classification problem, I mean that the solution is a mapping from the independent variables to a classification. Different classifications are generated by changing this mapping. Conversely, by optimisation problem, I mean that repeated solutions are "guessed" with the end result being iterated to determine the solution - potentially without regard to the input variables. This includes methods such as Broyden's Method (to find where the probability of correct classification is 100\%), Mixed Integer Linear Programming (accounting for the discrete values of many of the variables, including the survival), and other data-based optimisation (to maximise the probability of correct classification). 

Hence, if you find yourself making hundreds of submissions based on minor changes to the parameters of your classifier, I suggest you ask yourself *Am I solving a classification problem, or an optimisation problem on my score?*

## Scope

When starting a data science project, I like to take a moment to set expectations and boundaries. If you're working on a project, these may be stated in a business case, a contract, or some other formal document. 

It should further be noted that, often, these are seen as mere guidelines....

### In Scope

Within this kernel, I shall:

* Solve the classification problem using Support Vector Machines and various rule-based methods.  
* Compare the performance of these on the supplied training and test data sets.  
* Give examples of creating additional variables, implementation, and monitoring.


### Out of Scope

In this kernel, I shall not:

* Pursue external data sets. This includes the obvious "Look up the passenger manifest and survival" option.  
* Utilise other classification methods (e.g. decision trees, neural networks, etc). If you'd like to extend this work to include other methods, feel free to copy/fork/etc this kernel.  
* Build ensemble models. It is possible to "nest" SVMs - use a simple one to generate a prediction, include this prediction as an input to another SVM, etc. It's also possible to ensemble them, as in random forests: By using techniques from random forests, you can create a collection of SVMs and utilise voting to generate predictions. Again, if you'd like to try such techniques, feel free to copy/fork/etc this kernel and have a go.

## Library Load

I've loaded some libraries to aid in this kernel. Specifically:

1. `tidyverse`: Conveniently loads `dplyr`, `forcats`, `ggplot2` and others that I often use.   
2. `e1071`: A package for `svm` and related utilities.
3. `scales`: Convenience in terms of formatting percentages. In fact, I've used this package in the preamble, so if it's not installed you'll have discovered it by now.
4. `DT`: Based on `datatable.js`, this allows for some nice formatting of tables.
5. `googleVis`: A gauge is given as monitoring the metric of probability of correct classification.
6. `magrittr`: A package with extended pipe cases.

```{r libraryLoad}
# Tidyverse for most of the code
library( tidyverse )
# e1071 for svm (support vector machine) functions
library( e1071 )
# scales for percentage formatting
library( scales )
# DT for sample results
library( DT )
# googleVis for monitoring via gauge
library( googleVis )
# magrittr for the double pipe -- I'm a fan, but it's a bit controversial....
library( magrittr )
```

## Data Load

As is common for Kaggle competitions, there are training and test sets that have been pre-defined.

```{r dataLoad}
rawTrainData <- readr::read_csv( '../input/train.csv' ) %>%
  mutate( Source='Train' )
rawTestData <- readr::read_csv( '../input/test.csv' ) %>%
  mutate( Source='Test' )

# Combine the data for ease of later processing
rawData <- bind_rows( rawTrainData, rawTestData )
```

## Data Cleaning

The data have some missing values for Age, which I expect to be an important variable. I've replaced these missing values with zero.

Also, I suspect that `SibSp` and `Parch` are ordinal, rather than continuous. I've included logic to convert those to factors rather than integers.

```{r dataClean} 
cleanData <- rawData %>%
  # Change the class to a character ("C" class -> Character class)
  mutate( Cclass = as.character( Pclass ) ) %>%
  # Replace missing doubles with 0.0
  mutate_if( is.double, list( ~if_else( is.na( . ), 0.0, . ) ) ) %>%
  # Convert logicals to characters
  mutate_if( is_logical, as.character ) %>%
  # Convert characters to factors
  mutate_if( is_character, ~fct_explicit_na( f=., na_level='Missing' ) ) 
```

## Additional Variables

In general, I recommend consulting experts to determine if there are any additional variables or interactions to consider. An obvious (but unspirited) external data source would be the passenger manifest and survival from the Titanic. 

For some ideas about features, and a general exploratory data analysis (EDA), I highly recommend [the illustrative kernel](https://www.kaggle.com/headsortails/tidy-titarnic) by [Heads or Tails](https://www.kaggle.com/headsortails).

```{r variableAddition}
# Get the number of passengers on the same ticket
ticketData <- cleanData %>%
  group_by( Ticket ) %>%
  summarise( nPassengers=n() ) 

cleanData <- cleanData %>%
  mutate(
    # To convert the SVM to a classification problem, convert the outcome variable to a factor
    # I'm putting Survival first because I'd like to consider survival to be "default"
    # (Even if the results imply otherwise)
    Survived=factor( Survived, levels=c(1,0), labels=c('Yes','No') )
  # See the linked kernel for the motivation and EDA of these variables
  # These are directly inspired by that kernel
  # That said, I've converted the modular division to round, and make the log base-10
  , fclass = factor( round( log10( 1 + Fare ) )  ) # Recall there are some 0-fares
  # The next few include the use of forcats, which is included in the tidyverse
  # As coded, there needs to be at least 50 of the same title to be differentiated
  , title_orig=forcats::fct_lump_min( str_extract( Name, '[A-Z][a-z]*\\.' ), min=50 )
  # The Cabin, when not missing, seems to be (letter)(number) - extract the letter.
  # Calling this variable "deck" might (or might not) be a bit of fun with my Kiwi friends
  , deck=forcats::fct_lump( 
    # Extract the first letter - if it exists
    # Note the use of regular expressions -- see ?regexp for examples
    # Basically, this one checks if (at least) the first character in the string is a letter.
    # If so, it grabs the first letters, and converts them to a factor, explicitly including NA.
    forcats::fct_explicit_na( str_extract( Cabin, '^[[:alpha:]]{1,}' ), na_level = 'Missing' )
    , 5 )
) %>%
  left_join( ticketData ) %>%
  mutate( effFare = Fare / nPassengers )

# Re-split into training and test
trainData <- cleanData %>%
  filter( Source == 'Train' )

testData <- cleanData %>%
  filter( Source == 'Test' ) %>%
  # Survived flag means nothing for test data
  select( -Survived )
```

# Model Derivation


## Expert Models

Kaggle provides a baseline model for comparison and to check the submission process. Their rule is simple: *A passenger survived if and only if they were female*. Utilising this simple rule yields a metric of `r BASELINE_PCC`. Obviously, any model should be compared to this benchmark.

```{r baseModel}
baseTrain <- trainData %>%
  mutate( predSurv=if_else( Sex == 'female', 1, 0 ) )

baseTest <- testData %>%
  mutate( Survived = if_else( Sex == 'female', 1, 0 ) )
```


Of course, your stakeholders might have their own ruleset to classify input data. While this may seem unnecessary for whether someone survived the Titantic tragedy, it would be more important for serious business usage (e.g. approval for a mortgage, patient diagnosis, etc.).

## SVM Models{.tabset .tabset-fade .tabset-pills}

Support Vector Machines are a means of solving classification, regression, and anomaly detection problems. They were popular in the 1990's and early 2000's, leading to the question as to whether they would solve all problems or overhyped, but seem to have fallen out of favour[^1].

[^1]: This leads me to believe that the overhyped argument won. But it's also made me skeptical when people ask the same about deep learning, etc.

They are defined by solutions to a constrained optimisation problem, leveraging the rich mathematics from convex optimisation. Often, a so-called *kernel trick* is employed to simplify the process - there is further mathematics around what functions are admissible as such kernels. 

I've defined some general functions to be re-used for the different SVM models.

```{r svmModels}
fitSVM <- function( trainData
                    , formula='Survived ~ .'
                    , type='C-classification'
                    , kernel='radial'
                    , gamma=0.1 
                    , cost=10 
                    , ...)
{
  svmObject <- svm( formula=as.formula( formula )
                    , data=trainData
                    , type=type
                    , kernel=kernel
                    , cost=cost
                    , gamma=gamma 
                    # Turn off scaling
                    , scale=FALSE
                    # If there are NA values, return an error
                    , na.action=na.fail
                    # Convergence of the optimisation
                    , tolerance=1E-6 
                    , ...)
  
  return( svmObject )
}

augmentPredictions <- function( dataIn, svmObject ){
  dataOut <- dataIn %>%
    mutate( predSurv = predict( svmObject, newdata=dataIn ) )
  
  return( dataOut )
}

sampleResults <- function( augData
                           , nSample=10
                           , truthName=quo( Survived )
                           , predName=quo( predSurv ) ){
    sampleData <- augData %>%
      sample_n( nSample ) %>%
      mutate( Correct=(!! truthName) == (!! predName) ) 
    
    return( sampleData )
}

makeResultsTable <- function( tableData ){
  resultsTable <- DT::datatable( 
    tableData %>% mutate( Correct=as.character( Correct ) )
    # Turn off rownames
    , rownames = FALSE
 ) %>%
    formatRound( c('Fare', 'effFare'), digits=2 ) %>%
    formatStyle( 'Correct'
                 , target='row'
                 , color='white'
                 , backgroundColor=styleEqual(c('TRUE', 'FALSE'), c('darkgreen', 'red') ) )

  # The resulting table is in HTML
  return( resultsTable )
}
```

### Introductory Model

I've played with SVMs in previous jobs. Based on my memory, I tried using a cost ($C$) of $10$, and a $\gamma$ value of $0.1$. I checked this on a small sample of data to see if it (roughly) aligned with the predictions.

```{r svmSimpleFit, results='asis'}
simpleSVM <- fitSVM( trainData=trainData
                     , formula='Survived ~ Age + Sex + effFare + Cclass + deck' )
simpleTrainData <- augmentPredictions( trainData, simpleSVM )
simpleSample <- sampleResults( simpleTrainData ) %>%
  select( PassengerId, Pclass, Sex, Age, Fare, effFare, Survived, predSurv, Correct )

# The DT package is a nice table to visualise the results, and allows for formatting
simpleTable <- makeResultsTable( simpleSample )
simpleTable

simpleTest <- testData %>% mutate( Survived=predict( simpleSVM, newdata=. ) ) 
simpleOut <- simpleTest %>% select( PassengerId, Survived )
write.csv( simpleOut, file='simpleSVM.csv', row.names=FALSE )
```

### All Variables

Although it's unclear (to me) how the family, port, and fare may influence the model, the SVM can "determine" whether such variables are useful. My next step was to build a model with all of these variables.

Note that I've excluded the name and identifier variables as I'm not sure how to extend them to the test data.

```{r svmGeneralFit, results='asis'}
generalSVM <- fitSVM( trainData, formula='Survived ~ Age + 
                      Sex + 
                      effFare + 
                      Cclass + 
                      SibSp + 
                      Parch + 
                      deck + 
                      Embarked + 
                      nPassengers + 
                      fclass + 
                      title_orig' )

generalTrainData <- augmentPredictions( trainData, generalSVM )
generalSample <- sampleResults( generalTrainData ) %>%
  select( PassengerId, Pclass, Sex, Age, Fare, effFare, Survived, predSurv, Correct )

generalTable <- makeResultsTable( generalSample )
generalTable
generalTest <- testData %>% mutate( Survived=predict( generalSVM, newdata=. ) ) 
generalOut <- generalTest %>% select( PassengerId, Survived )
write.csv( generalOut, file='generalSVM.csv', row.names = FALSE )
```

### Tuning the SVM

Try tuning if you'd like. I did this *twice* using different parameter settings: Once had no change, and the second led to my current submission on the leaderboard. I recommend against running this if you're just after an introduction -- but if you have the time, feel free to change the ranges, settings, and have a go!

```{r tuneSVM, results='asis'}
# Set the random number seed so that this is repeatable
set.seed( 42 )
tuneSVM <- tune( svm
                 , Survived ~ Sex + 
                      Sex + 
                      effFare + 
                      Cclass + 
                      SibSp + 
                      Parch + 
                      deck + 
                      Embarked + 
                      nPassengers + 
                      fclass + 
                      title_orig
                 , data=trainData 
                 , type='C-classification'
                 , ranges=list( cost=10^seq( from=-1, to=3, by=0.5 )
                                , gamma=10^seq(from=-2, to=1, by=0.5 ) )
                 , tunecontrol=tune.control( sampling='boot' )
)

tuneTrainData <- augmentPredictions( generalTrainData %>% rename( predSurvGeneral=predSurv ), tuneSVM$best.model )
tuneTestData <- generalTest %>% 
  mutate( Survived2=predict( tuneSVM$best.model, newdata=. ) ) 
outData2 <- tuneTestData %>% 
  select( PassengerId, Survived=Survived2 )

write.csv( outData2, file='tunedSurvival.csv', row.names=FALSE )

# Have a look at what's different...
swapSet <- tuneTrainData %>% 
  filter( predSurvGeneral != predSurv ) %>%
  mutate( tunedCorrect = (predSurv == Survived ) ) %>%
  select( PassengerId
          , Sex
          , Cclass
          , effFare
          , Untuned=predSurvGeneral
          , Tuned=predSurv
          , tunedCorrect )

if( nrow( swapSet ) > 0 ){
  knitr::kable( swapSet %>% sample_n( 5 )
                , caption='Example differences between tuned and untuned SVMs' 
                , digits=2 )
}
```

### Nu ($\nu$) SVM

An attractive feature of SVM is the ability to explicity restrict the complexity. Whilst this may result in lesser performance, it has a number of positive aspects, including:   

1. It helps guard against overtraining, and  
2. It reduces complexity for implementation purposes.

```{r nuSVM, results='asis'}
nuSVM <- fitSVM( trainData
                 , formula='Survived ~ Age + 
                      Sex + 
                      effFare + 
                      Cclass + 
                      deck + 
                      Embarked + 
                      nPassengers + 
                      fclass'
                 , type='nu-classification'
                 , nu=0.005 )

nuTrainData <- augmentPredictions( trainData, nuSVM )
nuSample <- sampleResults( nuTrainData ) %>%
  select( PassengerId, Pclass, Sex, Age, Fare, effFare, Survived, predSurv, Correct )

nuTable <- makeResultsTable( nuSample )
nuTable

testDataNu <- testData %>% mutate( Survived=predict( nuSVM, newdata=. ) ) 
outDataNu <- testDataNu %>% select( PassengerId, Survived )
write.csv( outDataNu, file='nuSurvival.csv', row.names = FALSE )
```


# Recommendations{.tabset .tabset-fade .tabset-pills}

I have two groups of recommendations: 

1. **Implementation**: While building the model and generating predictions is straightforward in an evironment built for statistical programming, many organisations have different (or *very* different) production environments. Along with the model, it is good practice to include instructions and test cases so that there is assurance that the implemented model is working as intended.
Note there's a trade-off in performance versus complexity. Whether or not the improved performance is "worth" the complexity is a business decision that should be considered carefully.  
2. **Monitoring**: Once the model is implemented, the users of the model should have assurance that the model continues to work correctly. How much variation in model performance is "normal"? Is model performance better (or worse) in certain situations? With many machine learning models, it is difficult to assess what external factors influence prediction accuracy.  


## Implementation

Before deciding upon which model to implement, let's consider the complexity and performance of the SVM models created:

```{r summFunc}
getTrainPerf <- function( metricData ){
  trainPerf <- with( metricData, mean( as.numeric( predSurv == Survived ) ) )
  return( trainPerf )
}
```

Model | Number of Variables | Number of Support Vectors | Training Perf | Test Perf |
------|---------------------|---------------------------|---------------|-----------|
Baseline | 1 | **N/A** | `r scales::percent( getTrainPerf( baseTrain ) )` | `r BASELINE_PCC`
Small | 5 | `r simpleSVM$tot.nSV` | `r scales::percent(getTrainPerf( simpleTrainData ) )` |  `r SVM_SMALL_PCC`
Full | 8 | `r generalSVM$tot.nSV` | `r scales::percent( getTrainPerf( generalTrainData ) )` |  `r SVM_PCC1`
Nu | 8 | `r nuSVM$tot.nSV` | `r scales::percent( getTrainPerf( nuTrainData ) )` |  Z

For illustrative purposes, I'll consider a prediction from the $\nu$ regression. If this seems arduous, consider that when passing this off to the implementation team!

### Derived Variables

There are some derived variables, (e.g. `deck`), but the code - and therefore the logic - is included above.

### Dummy variables

Let's manually go through and mutate in the factor variables.

```{r dummyVars}
# Get a data frame of the required fields from the support vectors
svDF <- as.data.frame( nuSVM$SV )

# It does seem like there should be a better way to do this, but here we go!
svTrainData <- trainData %>%
  mutate( Sexfemale = if_else( Sex == 'female', 1, 0 )
          , Sexmale = if_else( Sex == 'male', 1, 0 ) 
          , Cclass2 = if_else( Cclass == '2', 1, 0 )
          , Cclass3 = if_else( Cclass == '3', 1, 0 )
          , deckC = if_else( deck == 'C', 1, 0 )
          , deckD = if_else( deck == 'D', 1, 0 )
          , deckE = if_else( deck == 'E', 1, 0 )
          , deckMissing = if_else( deck == 'Missing', 1, 0 )
          , deckOther = if_else( deck == 'Other', 1, 0 )
          , EmbarkedQ = if_else( Embarked == 'Q', 1, 0 )
          , EmbarkedS = if_else( Embarked == 'S', 1, 0 )
          , EmbarkedMissing = if_else( Embarked == 'Missing', 1, 0 )
          , fclass1 = if_else( fclass == '1', 1, 0 )
          , fclass2 = if_else( fclass == '2', 1, 0 )
          , fclass3 = if_else( fclass == '3', 1, 0 )
  ) %>%
  # Down-select to only the names required
  select( names( svDF ) )
```

### Scaling

Often, the independent variables are scaled to better normalise results. However, in the defined function, I've disabled this feature - in part because I didn't want to go through the process of documenting and checking the scaling. If you'd like to approach the problem from a more robust standpoint, feel free to enable the scaling in the `fitSVM` function and include the resulting scale parameters here.



Take these values and transform them, via the kernel, into decision space.

```{r kernelFunction}
radialKernel <- function( x, supportVectors, gamma=0.1 ){
  x1 <- x[rep(1, nrow( supportVectors) ), ]
  y <- exp( -gamma * rowSums( (x1 - supportVectors)^2 ) )
  return( y )
}
```

Let's check the decision values. Yes, all of them....

```{r dvCheck}
N <- nrow( svTrainData )
myDV <- rep( 0, N )
for( passengerIdx in 1:N ){
  myDV[passengerIdx] <- sum( radialKernel( svTrainData[passengerIdx,], svDF )*nuSVM$coefs ) - 
    nuSVM$rho
}
# Check
dvCheck <- max( abs( myDV - nuSVM$decision.values ) )
```

I consider a values of `r dvCheck` to be "close enough"....

The algorithm thus considers anything with a negative `decision.value` to be survival - which is a bit of an unfortunate sign convention.

```{r  impClassTest}
impClassTest <- nuTrainData %>% 
  bind_cols( data.frame( DV=myDV ) ) %>%
  mutate( testClass = factor( if_else( DV < 0, 1, 0 )
                              # Mirroring the factor definition from input data
                              , levels=c(1,0)
                              , labels=c('Yes','No') )
  )

# Obviously, the names are different...
all.equal( impClassTest$predSurv, impClassTest$testClass, check.names=FALSE )
```

For implementation purposes, we thus require:  
1. The mapping to dummy variables, as in the big `mutate` block.  
2. A list of the support vectors and their corresponding weights, from the relevant `SVM` object. Whether or not the implementation team complains about `r nuSVM$tot.nSV` support vectors, each of `r ncol( svDF )` is an interesting question!  
3. The intercept, stored as `rho` in the `SVM` object.  
4. The kernel function, as in `radialKernel`.  
5. The mapping from `decision.values` to survival prediction (i.e. Which does positive indicate?).

I'd likely build a big Excel document with all that information, including intermediate calculations and some examples. Unfortunately, including a fat table in the context of a Kaggle kernel isn't really practical.

## Monitoring

Whilst this particular problem has a single application, in general models will require monitoring to ensure their ongoing performance is aligned with its performance in development. One method to do this is with a gauge. These are quite simple, and provide a good means to quickly visualise performance of a continuous value. In this case, it's the probability of correct classification.

This requires feedback about the limits of what should be considered *good*, *marginal* and *unsatisfactory* performance levels. For sake of argument, let's say that performance of over 80\% is considered *good*, over 70\% is marginal, and below that is *unsatisfactory*.

If there were additional metrics - for instance, the PCC for different segments -- these could be extended in the gauges. As an example, I've split the data into components of Overall, Men, and Women.

I don't know how my chosen model will work on the private test data, so whether or not my model will be rated green on the private data remains to be seen!

```{r gaugeVis, results='asis'}
MAX_GREEN = 1
MIN_GREEN = 0.8
MAX_YELLOW = MIN_GREEN
MIN_YELLOW = 0.7
MAX_RED = MIN_YELLOW
MIN_RED = 0

gaugeDF <- # Get the metrics....
  tibble( Overall=getTrainPerf( nuTrainData )
                       , Men=getTrainPerf( nuTrainData %>% filter( Sex == 'male' ) )
                       , Women=getTrainPerf( nuTrainData %>% filter( Sex == 'female' ) ) 
              )%>%
  # Transform from fat & wide to thin & long
  tidyr::gather( key=Metric, value=PCC ) %>%
  # googleVis wants this to be a data frame
  as.data.frame()

myGauge <- gvisGauge( gaugeDF
                      , labelvar='Metric'
                      , numvar = 'PCC'
                      , options=list( min=0, max=1
                                      , greenFrom=MIN_GREEN, greenTo=MAX_GREEN
                                      , yellowFrom=MIN_YELLOW, yellowTo=MAX_YELLOW
                                      , redFrom=MIN_RED, redTo=MAX_RED )
                      )

print( myGauge, tag='chart' ) 
```


# Conclusions


# Acknowledgements

In building this, I'd like to thank the following:

1. [Kaggle](https://www.kaggle.com), for hosting the data and the [open competition](https://www.kaggle.com/c/titanic).

2. `jrowan` for the [`rhandsontable` package, and its documentation](http://jrowen.github.io/rhandsontable/). Note that I couldn't get this working on Kaggle, so I switched to `DT`....

2a. RStudio for [releasing and documenting the DT package](https://rstudio.github.io/DT/).

3. [Heads or Tails](https://www.kaggle.com/headsortails) for their [public kernel](https://www.kaggle.com/headsortails/tidy-titarnic).

3. giphy.com for making animated GIFs for extra pizzazz in this kernel.

# Appendix

## Session Info

The `sessionInfo()` used in the creation of this document is below. It's a place to start if there are difficulties reproducing this report (i.e. check `R` version, package versions, etc.).

```{r sessionInfo}
sessionInfo()
```